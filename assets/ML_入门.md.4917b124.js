import{_ as a,c as i,o as e,a as t}from"./app.58320140.js";const l=JSON.parse('{"title":"机器学习入门 -- 常用算法介绍","description":"","frontmatter":{},"headers":[],"relativePath":"ML/入门.md"}'),s={name:"ML/入门.md"},r=t('<h1 id="机器学习入门-常用算法介绍" tabindex="-1">机器学习入门 -- 常用算法介绍 <a class="header-anchor" href="#机器学习入门-常用算法介绍" aria-hidden="true">#</a></h1><p><img src="https://upload-images.jianshu.io/upload_images/5415189-e66260dd37d757ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>先介绍几个概念： 决策树，朴素贝叶斯，梯度下降，线性回归</p><h6 id="决策树" tabindex="-1">决策树 <a class="header-anchor" href="#决策树" aria-hidden="true">#</a></h6><p><img src="https://upload-images.jianshu.io/upload_images/5415189-f37a38f7e9df78e0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="决策树"></p><h6 id="朴素贝叶斯" tabindex="-1">朴素贝叶斯 <a class="header-anchor" href="#朴素贝叶斯" aria-hidden="true">#</a></h6><p>这里我只想到贝叶斯的概率公式，欢迎补充。 <img src="https://upload-images.jianshu.io/upload_images/5415189-65e0af1eb1734358.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="朴素贝叶斯"></p><h6 id="梯度下降" tabindex="-1">梯度下降 <a class="header-anchor" href="#梯度下降" aria-hidden="true">#</a></h6><p>记得第一次看见这个概念是在多元微积分里，这次捡起来。 梯度下降用来最小化误差。 <img src="https://upload-images.jianshu.io/upload_images/5415189-f4b1b21b270a9c06.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="梯度下降"></p><h6 id="线性回归" tabindex="-1">线性回归 <a class="header-anchor" href="#线性回归" aria-hidden="true">#</a></h6><p>提到这个直接想到的就是最小二乘法，通过散点拟合线性方程。 <img src="https://upload-images.jianshu.io/upload_images/5415189-fe351752fec3c1ae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="线性回归"></p><p>接下来是机器学习常用的算法：</p><h6 id="逻辑回归" tabindex="-1">逻辑回归 <a class="header-anchor" href="#逻辑回归" aria-hidden="true">#</a></h6><p><img src="https://upload-images.jianshu.io/upload_images/5415189-e2fab8ee953c815b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="逻辑回归"></p><p>######SVM(支持向量机) <img src="https://upload-images.jianshu.io/upload_images/5415189-53a5d8769f78fe3a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="SVM"></p><p>######核函数 <img src="https://upload-images.jianshu.io/upload_images/5415189-31d2141cfc4b716f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="核函数"></p><p>######神经网络 <img src="https://upload-images.jianshu.io/upload_images/5415189-1315d116fe1ca629.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="神经网络"><img src="https://upload-images.jianshu.io/upload_images/5415189-e439bcd7f1186834.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="神经网络类比生物神经元"></p><p>######K均值聚类 如果数据有很多堆呢？ <img src="https://upload-images.jianshu.io/upload_images/5415189-2c630e226766faf8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="K聚类"></p><h6 id="层次聚类" tabindex="-1">层次聚类 <a class="header-anchor" href="#层次聚类" aria-hidden="true">#</a></h6><p>如果不知道k聚类分成多少堆，这时候用层次聚类再合适不过了。 <img src="https://upload-images.jianshu.io/upload_images/5415189-65a6b5e58d3c61e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="层次聚类"></p><h4 id="总结" tabindex="-1">总结： <a class="header-anchor" href="#总结" aria-hidden="true">#</a></h4><p>可以说逻辑回归是一个把数据分成部分的忍者，而SVM又一个对周围的数据比较挑剔。 <img src="https://upload-images.jianshu.io/upload_images/5415189-693919b7a72cb537.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="逻辑回归和SVM"> 神经网络是一个忍者群，核函数让不同的东西上升或者下降。 <img src="https://upload-images.jianshu.io/upload_images/5415189-05c9155c69252da1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="神经网络和核函数"></p>',22),o=[r];function p(g,d,n,h,m,u){return e(),i("div",null,o)}const _=a(s,[["render",p]]);export{l as __pageData,_ as default};
